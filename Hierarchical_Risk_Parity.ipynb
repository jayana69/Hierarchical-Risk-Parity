{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Portfolio Allocation Using Hierarchial Risk Parity ([Explained Here](https://www.youtube.com/watch?v=9MSPeAYBYIY))\n",
        "HRP portfolios tackle three primary issues commonly associated with quadratic optimizers, especially in the context of Markowitz's Critical Line Algorithm (CLA). These issues are instability, concentration, and underperformance. Monte Carlo simulations demonstrate that HRP achieves reduced out-of-sample variance compared to CLA, despite CLA's primary focus on minimizing variance. Furthermore, HRP generates portfolios with lower out-of-sample risk when compared to conventional risk parity techniques.\n",
        "\n",
        "\n",
        "\n",
        "You have two options. You can manually create a CSV file with daily returns data from [NASDAQ Historical Data](https://www.nasdaq.com/market-activity/quotes/historical) (This is tedious). Or use the first block of code to request monthly returns data from an API. Manually creating a CSV file is the best option because it provides 10 years of daily data. API only provides a couple years of monthly data.\n",
        "\n"
      ],
      "metadata": {
        "id": "GyGJiuJ3ZKeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Monthly Returns:\n",
        "1. Run the first block of code. You'll have to enter 10 stocks. **Record the order you enter the stocks.** The function will gather monthly returns data from the API. The API can only handle 5 request per minute so the function will take about 2 minutes to run. If you pay me I can cop a better subscription 🙏\n",
        "2. After a dataframe with monthly returns data is recieved, run the third block of code (HRP). The weights (percentages in decimal format) will be given."
      ],
      "metadata": {
        "id": "6OVkmr6EyXll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from numpy.lib.function_base import append\n",
        "\n",
        "# Insert Personal Key\n",
        "apiKey = \"\"\n",
        "symbols = []\n",
        "\n",
        "def portfolio_input():\n",
        "  for i in range(1, 11):\n",
        "     stock = input(f\"Enter Stock {i}: \")\n",
        "     symbols.append(stock)\n",
        "portfolio_input()\n",
        "\n",
        "def calculate_monthly_returns(symbol):\n",
        "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY&symbol={symbol}&apikey={apiKey}'\n",
        "\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "\n",
        "    closing_prices = [float(data[\"Monthly Time Series\"][date][\"4. close\"]) for date in data[\"Monthly Time Series\"]]\n",
        "\n",
        "    monthly_returns = []\n",
        "\n",
        "    for i in range(1, len(closing_prices)):\n",
        "        current_price = closing_prices[i]\n",
        "        previous_price = closing_prices[i - 1]\n",
        "        monthly_return = (current_price - previous_price) / previous_price\n",
        "        monthly_returns.append(monthly_return)\n",
        "\n",
        "    dates = list(data[\"Monthly Time Series\"].keys())[1:]\n",
        "    df = pd.DataFrame({len(dataframes) + 1: monthly_returns})\n",
        "\n",
        "    return df\n",
        "\n",
        "dataframes = []\n",
        "for symbol in symbols:\n",
        "    df = calculate_monthly_returns(symbol)\n",
        "    if df.empty:\n",
        "        break\n",
        "    dataframes.append(df)\n",
        "    if len(dataframes) % 5 == 0:\n",
        "        print(\"Waiting for 62 seconds...\")\n",
        "        time.sleep(62)\n",
        "\n",
        "if dataframes:\n",
        "    merged_df = pd.concat(dataframes, axis=1)\n",
        "    merged_df.dropna(inplace=True)\n",
        "\n",
        "    csv_filename = 'monthly_returns_official.csv'\n",
        "    merged_df.to_csv(csv_filename, index=True, header=False)\n",
        "    print(f'Data saved as {csv_filename}')\n",
        "    print(merged_df)\n",
        "else:\n",
        "    print(\"No data available for all stocks.\")\n"
      ],
      "metadata": {
        "id": "ymDhR2p-tClO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merger\n",
        "**The following code will merge the ten CSV files you downloaded off of NASDAQ into one CSV file for the HRP program**\n",
        "\n",
        "Put all ten csv files you downloaded off of NASDAQ into one folder and put the path to it here:\n",
        "\n",
        "\n",
        "```\n",
        "csv_directory = 'returnsFiles'\n",
        "```\n",
        "Then specify where you want the merged file to save:\n",
        "```\n",
        "merged_df.to_csv('output/diverseReturns.csv')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iw4gCrJnyjRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "merged_df = pd.DataFrame()\n",
        "\n",
        "csv_directory = '/content/seventyFivePercent'\n",
        "\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        stock_data = pd.read_csv(os.path.join(csv_directory, filename))\n",
        "\n",
        "        stock_name = filename.split('.')[0]\n",
        "        stock_data['Daily_Return_' + stock_name] = (stock_data['Close'] - stock_data['Open']) / stock_data['Open']\n",
        "\n",
        "        if merged_df.empty:\n",
        "            merged_df = stock_data[['Date', 'Daily_Return_' + stock_name]]\n",
        "        else:\n",
        "            merged_df = pd.merge(merged_df, stock_data[['Date', 'Daily_Return_' + stock_name]], on='Date', how='outer')\n",
        "\n",
        "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
        "merged_df.set_index('Date', inplace=True)\n",
        "merged_df.to_csv('seventyFivePercentMerged')\n",
        "\n",
        "print(merged_df.head())\n"
      ],
      "metadata": {
        "id": "NWpyE5VeyiqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchial Risk Parity\n",
        "\n",
        "Make sure you replace the path:\n",
        "\n",
        "\n",
        "```\n",
        "csv_path = '/content/sample_data/diverseReturns.csv'\n",
        "```\n",
        "\n",
        "To the correct relative path to your historical returns data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6YItVmQYyABM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as mpl\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def getIVP(cov, **kargs):\n",
        "    # Compute the inverse-variance portfolio\n",
        "    ivp = 1. / np.diag(cov)\n",
        "    ivp /= ivp.sum()\n",
        "    return ivp\n",
        "\n",
        "def getClusterVar(cov, cItems):\n",
        "    # Compute variance per cluster\n",
        "    cov_ = cov.loc[cItems, cItems] # matrix slice\n",
        "    w_ = getIVP(cov_).reshape(-1, 1)\n",
        "    cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]\n",
        "    return cVar\n",
        "\n",
        "def getQuasiDiag(link):\n",
        "    # Sort clustered items by distance\n",
        "    link = link.astype(int)\n",
        "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
        "    numItems = link[-1, 3] # number of original items\n",
        "    while sortIx.max() >= numItems:\n",
        "        sortIx.index = range(0, sortIx.shape[0] * 2, 2) # make space\n",
        "        df0 = sortIx[sortIx >= numItems]\n",
        "        i = df0.index\n",
        "        j = df0.values - numItems\n",
        "        sortIx[i] = link[j, 0] # item 1\n",
        "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
        "        sortIx = pd.concat([sortIx, df0]) # item 2 (modified to use concat)\n",
        "        sortIx = sortIx.sort_index() # re-sort\n",
        "        sortIx.index = range(sortIx.shape[0]) # re-index\n",
        "    return sortIx.tolist()\n",
        "\n",
        "\n",
        "def getRecBipart(cov, sortIx):\n",
        "    # Compute HRP allocation\n",
        "    w = pd.Series(1, index=sortIx)\n",
        "    cItems = [sortIx] # initialize all items in one cluster\n",
        "    while len(cItems) > 0:\n",
        "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
        "        for i in range(0, len(cItems), 2): # parse in pairs\n",
        "            cItems0 = cItems[i] # cluster 1\n",
        "            cItems1 = cItems[i + 1] # cluster 2\n",
        "            cVar0 = getClusterVar(cov, cItems0)\n",
        "            cVar1 = getClusterVar(cov, cItems1)\n",
        "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
        "            w[cItems0] *= alpha # weight 1\n",
        "            w[cItems1] *= 1 - alpha # weight 2\n",
        "    return w\n",
        "\n",
        "def correlDist(corr):\n",
        "    # Compute the correlation distance\n",
        "    dist = ((1 - corr) / 2.)**.5 # distance matrix\n",
        "    return dist\n",
        "\n",
        "def plotCorrMatrix(path, corr, labels=None):\n",
        "    # Heatmap of the correlation matrix\n",
        "    if labels is None:\n",
        "        labels = []\n",
        "    mpl.pcolor(corr)\n",
        "    mpl.colorbar()\n",
        "    mpl.yticks(np.arange(.5, corr.shape[0] + .5), labels)\n",
        "    mpl.xticks(np.arange(.5, corr.shape[0] + .5), labels)\n",
        "    mpl.savefig(path)\n",
        "    mpl.clf()\n",
        "    mpl.close() # reset pylab\n",
        "    return\n",
        "\n",
        "def generateDataFromCSV(csv_path):\n",
        "    # Read the data from the CSV file\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    return data\n",
        "\n",
        "def main():\n",
        "    nObs, size0, size1, sigma1 = 10000, 5, 5, .25\n",
        "\n",
        "    # Provide the path to your CSV file\n",
        "    csv_path = '/content/seventyFivePercentMerged'\n",
        "\n",
        "    # Call the modified generateData function\n",
        "    data = generateDataFromCSV(csv_path)\n",
        "    cols = [random.randint(0, size0 - 1) for i in range(size1)]\n",
        "    print([(j + 1, size0 + i) for i, j in enumerate(cols, 1)])\n",
        "    # 2) Compute and plot correl matrix\n",
        "    cov, corr = data.cov(), data.corr()\n",
        "    plotCorrMatrix('HRP3_corr0.png', corr, labels=corr.columns)\n",
        "    # 3) Cluster\n",
        "    dist = correlDist(corr)\n",
        "    link = sch.linkage(dist, 'single')\n",
        "    sortIx = getQuasiDiag(link)\n",
        "    sortIx = corr.index[sortIx].tolist() # recover labels\n",
        "    df0 = corr.loc[sortIx, sortIx] # reorder\n",
        "    plotCorrMatrix('HRP3_corr1.png', df0, labels=df0.columns)\n",
        "    # 4) Capital allocation\n",
        "    hrp = getRecBipart(cov, sortIx)\n",
        "\n",
        "    return print(hrp)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "s_G3G5mykYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the [backtester](https://www.portfoliovisualizer.com/backtest-portfolio#analysisResults) to test performance."
      ],
      "metadata": {
        "id": "Hu69McM-JZsN"
      }
    }
  ]
}