{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Portfolio Allocation Using Hierarchial Risk Parity ([Explained Here](https://www.youtube.com/watch?v=9MSPeAYBYIY))\n",
        "HRP portfolios tackle three primary issues commonly associated with quadratic optimizers, especially in the context of Markowitz's Critical Line Algorithm (CLA). These issues are instability, concentration, and underperformance. Monte Carlo simulations demonstrate that HRP achieves reduced out-of-sample variance compared to CLA, despite CLA's primary focus on minimizing variance. Furthermore, HRP generates portfolios with lower out-of-sample risk when compared to conventional risk parity techniques.\n",
        "\n",
        "\n",
        "\n",
        "You have two options. You can manually create a CSV file with daily returns data from [NASDAQ Historical Data](https://www.nasdaq.com/market-activity/quotes/historical) (This is tedious). Or use the first block of code to request monthly returns data from an API. Manually creating a CSV file is the best option because it provides 10 years of daily data. API only provides a couple years of monthly data.\n",
        "\n"
      ],
      "metadata": {
        "id": "GyGJiuJ3ZKeE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API Monthly Returns:\n",
        "1. Run the first block of code. You'll have to enter 10 stocks. **Record the order you enter the stocks.** The function will gather monthly returns data from the API. The API can only handle 5 request per minute so the function will take about 2 minutes to run. If you pay me I can cop a better subscription üôè\n",
        "2. After a dataframe with monthly returns data is recieved, run the third block of code (HRP). The weights (percentages in decimal format) will be given."
      ],
      "metadata": {
        "id": "6OVkmr6EyXll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "from numpy.lib.function_base import append\n",
        "\n",
        "# Insert Personal Key\n",
        "apiKey = \"\"\n",
        "symbols = []\n",
        "\n",
        "def portfolio_input():\n",
        "  for i in range(1, 11):\n",
        "     stock = input(f\"Enter Stock {i}: \")\n",
        "     symbols.append(stock)\n",
        "portfolio_input()\n",
        "\n",
        "def calculate_monthly_returns(symbol):\n",
        "    url = f'https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY&symbol={symbol}&apikey={apiKey}'\n",
        "\n",
        "    r = requests.get(url)\n",
        "    data = r.json()\n",
        "\n",
        "    closing_prices = [float(data[\"Monthly Time Series\"][date][\"4. close\"]) for date in data[\"Monthly Time Series\"]]\n",
        "\n",
        "    monthly_returns = []\n",
        "\n",
        "    for i in range(1, len(closing_prices)):\n",
        "        current_price = closing_prices[i]\n",
        "        previous_price = closing_prices[i - 1]\n",
        "        monthly_return = (current_price - previous_price) / previous_price\n",
        "        monthly_returns.append(monthly_return)\n",
        "\n",
        "    dates = list(data[\"Monthly Time Series\"].keys())[1:]\n",
        "    df = pd.DataFrame({len(dataframes) + 1: monthly_returns})\n",
        "\n",
        "    return df\n",
        "\n",
        "dataframes = []\n",
        "for symbol in symbols:\n",
        "    df = calculate_monthly_returns(symbol)\n",
        "    if df.empty:\n",
        "        break\n",
        "    dataframes.append(df)\n",
        "    if len(dataframes) % 5 == 0:\n",
        "        print(\"Waiting for 62 seconds...\")\n",
        "        time.sleep(62)\n",
        "\n",
        "if dataframes:\n",
        "    merged_df = pd.concat(dataframes, axis=1)\n",
        "    merged_df.dropna(inplace=True)\n",
        "\n",
        "    csv_filename = 'monthly_returns_official.csv'\n",
        "    merged_df.to_csv(csv_filename, index=True, header=False)\n",
        "    print(f'Data saved as {csv_filename}')\n",
        "    print(merged_df)\n",
        "else:\n",
        "    print(\"No data available for all stocks.\")\n"
      ],
      "metadata": {
        "id": "ymDhR2p-tClO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "851fac49-80c7-4d47-f2ce-6ed53eb29ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Stock 1: aapl\n",
            "Enter Stock 2: msft\n",
            "Enter Stock 3: tsla\n",
            "Enter Stock 4: intc\n",
            "Enter Stock 5: googl\n",
            "Enter Stock 6: amzn\n",
            "Enter Stock 7: lulu\n",
            "Enter Stock 8: gme\n",
            "Enter Stock 9: pwsc\n",
            "Enter Stock 10: meta\n",
            "Waiting for 62 seconds...\n",
            "Waiting for 62 seconds...\n",
            "Data saved as monthly_returns_official.csv\n",
            "          1         2         3         4          5          6         7   \\\n",
            "0   0.078411 -0.024698 -0.048728 -0.092224  -0.003950  -0.047221 -0.019822   \n",
            "1   0.045670  0.024896  0.036229  0.017928  -0.025336  -0.031375 -0.007160   \n",
            "2  -0.012624  0.013753 -0.021164 -0.065138  -0.098101  -0.024835 -0.000079   \n",
            "3  -0.086199 -0.035679 -0.220957 -0.059809   0.026483  -0.075023 -0.123038   \n",
            "4  -0.042708 -0.064344 -0.194282 -0.012087  -0.126394  -0.125477  0.144609   \n",
            "5  -0.028171 -0.061707  0.262613  0.051835  -0.033631  -0.020484 -0.041429   \n",
            "6  -0.106064 -0.134860 -0.008435 -0.236915  -0.131784  -0.087714 -0.150993   \n",
            "7  -0.021165 -0.006455 -0.157941  0.133574   0.097491   0.094450 -0.007503   \n",
            "8  -0.099522 -0.032242 -0.288881 -0.064756  -0.107345  -0.185494  0.043991   \n",
            "9   0.139306  0.063881  0.580614  0.137722   0.144622   0.149286  0.187059   \n",
            "10  0.035871 -0.090186  0.168670 -0.054539  -0.064165   0.061115 -0.134811   \n",
            "11 -0.098735  0.003317  0.165729 -0.093563   0.012062   0.103085 -0.150377   \n",
            "12  0.137627  0.122671  0.039057  0.238650   0.131417   0.121858  0.072972   \n",
            "13  0.033647  0.073699  2.234462  0.137531   0.074848   0.064526  0.035171   \n",
            "14 -0.158698 -0.085168 -0.244579  0.030295  17.735041  -0.212968 -0.122057   \n",
            "15  0.088648  0.058560  0.125984  0.187383   0.044042  21.636192  0.073658   \n",
            "16  0.059191  0.020782  0.148366 -0.018685   0.003055   0.033874  0.211623   \n",
            "17  0.107580  0.110947  0.237540  0.136958   0.218720   0.311519  0.029890   \n",
            "18 -0.054350 -0.030878 -0.192251 -0.037530  -0.028839  -0.057881 -0.124004   \n",
            "19  0.058503  0.040798  0.076158  0.023480   0.001825  -0.025980  0.043196   \n",
            "20  0.015963  0.081484  0.128171  0.054896   0.070571   0.114616  0.172849   \n",
            "21 -0.069100 -0.017037  0.083253 -0.044660  -0.020397   0.051803  0.160838   \n",
            "22 -0.093769  0.003116 -0.026870 -0.004065   0.043331  -0.038391  0.025528   \n",
            "23 -0.055407 -0.149870 -0.303878  0.087347  -0.097064  -0.025913 -0.131564   \n",
            "24  0.073004  0.070800 -0.051271  0.014640   0.082449   0.056544 -0.011193   \n",
            "\n",
            "          8         9         10  \n",
            "0   0.041550  0.019825 -0.030060  \n",
            "1   0.196765  0.092676  0.076751  \n",
            "2   0.092342 -0.208109 -0.099247  \n",
            "3  -0.008247 -0.010449 -0.077566  \n",
            "4  -0.197921  0.102429 -0.092173  \n",
            "5   0.193364 -0.050766 -0.118093  \n",
            "6  -0.164639  0.157921 -0.174578  \n",
            "7   0.137285 -0.018736 -0.148451  \n",
            "8  -0.155921  0.024867 -0.192186  \n",
            "9   0.419827 -0.115685 -0.018614  \n",
            "10  0.080122 -0.020088 -0.211177  \n",
            "11 -0.112328 -0.165500  0.456419  \n",
            "12  0.139674  0.078490  0.200840  \n",
            "13  0.187500 -0.199444 -0.023507  \n",
            "14  2.596001 -0.163775  0.013514  \n",
            "15  0.019951  0.061411  0.200868  \n",
            "16  0.002646  0.183737  0.035272  \n",
            "17  0.331894  0.090489  0.109193  \n",
            "18 -0.259575 -0.046033 -0.050953  \n",
            "19 -0.116832  0.040000  0.484433  \n",
            "20  0.362251  0.005495  0.073709  \n",
            "21  0.322259  0.217365 -0.035350  \n",
            "22 -0.064727  0.152618 -0.002743  \n",
            "23 -0.043812  0.064907  0.048892  \n",
            "24  0.243745  0.401869  0.117829  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merger\n",
        "**The following code will merge the ten CSV files you downloaded off of NASDAQ into one CSV file for the HRP program**\n",
        "\n",
        "Put all ten csv files you downloaded off of NASDAQ into one folder and put the path to it here:\n",
        "\n",
        "\n",
        "```\n",
        "csv_directory = 'returnsFiles'\n",
        "```\n",
        "Then specify where you want the merged file to save:\n",
        "```\n",
        "merged_df.to_csv('output/diverseReturns.csv')\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "iw4gCrJnyjRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "merged_df = pd.DataFrame()\n",
        "\n",
        "csv_directory = '/content/seventyFivePercent'\n",
        "\n",
        "for filename in os.listdir(csv_directory):\n",
        "    if filename.endswith(\".csv\"):\n",
        "        stock_data = pd.read_csv(os.path.join(csv_directory, filename))\n",
        "\n",
        "        stock_name = filename.split('.')[0]\n",
        "        stock_data['Daily_Return_' + stock_name] = (stock_data['Close'] - stock_data['Open']) / stock_data['Open']\n",
        "\n",
        "        if merged_df.empty:\n",
        "            merged_df = stock_data[['Date', 'Daily_Return_' + stock_name]]\n",
        "        else:\n",
        "            merged_df = pd.merge(merged_df, stock_data[['Date', 'Daily_Return_' + stock_name]], on='Date', how='outer')\n",
        "\n",
        "merged_df['Date'] = pd.to_datetime(merged_df['Date'])\n",
        "merged_df.set_index('Date', inplace=True)\n",
        "merged_df.to_csv('seventyFivePercentMerged')\n",
        "\n",
        "print(merged_df.head())\n"
      ],
      "metadata": {
        "id": "NWpyE5VeyiqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5f56cd-4518-4f2a-f311-bd31f27c0d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Daily_Return_HCLTECH  Daily_Return_EWT  Daily_Return_DAL  \\\n",
            "Date                                                                   \n",
            "2002-08-12              0.046230         -0.017977               NaN   \n",
            "2002-08-13             -0.038144          0.001143               NaN   \n",
            "2002-08-14             -0.009952          0.028603               NaN   \n",
            "2002-08-15              0.000000          0.034141               NaN   \n",
            "2002-08-16             -0.000794          0.006487               NaN   \n",
            "\n",
            "            Daily_Return_INFY  Daily_Return_GD  Daily_Return_LULU  \\\n",
            "Date                                                                \n",
            "2002-08-12                0.0         0.012070                NaN   \n",
            "2002-08-13                0.0        -0.041843                NaN   \n",
            "2002-08-14                0.0        -0.014295                NaN   \n",
            "2002-08-15                0.0        -0.031726                NaN   \n",
            "2002-08-16                0.0         0.031878                NaN   \n",
            "\n",
            "            Daily_Return_RMV  Daily_Return_MSFT  Daily_Return_EWZ  \\\n",
            "Date                                                                \n",
            "2002-08-12               NaN           0.018063         -0.053483   \n",
            "2002-08-13               NaN          -0.019379         -0.004060   \n",
            "2002-08-14               NaN           0.052063         -0.008097   \n",
            "2002-08-15               NaN          -0.003803         -0.033784   \n",
            "2002-08-16               NaN           0.011531          0.062937   \n",
            "\n",
            "            Daily_Return_IEMG  Daily_Return_ESGV  Daily_Return_V (1)  \\\n",
            "Date                                                                   \n",
            "2002-08-12                NaN                NaN                 NaN   \n",
            "2002-08-13                NaN                NaN                 NaN   \n",
            "2002-08-14                NaN                NaN                 NaN   \n",
            "2002-08-15                NaN                NaN                 NaN   \n",
            "2002-08-16                NaN                NaN                 NaN   \n",
            "\n",
            "            Daily_Return_UAA  Daily_Return_FL  Daily_Return_UPS  \n",
            "Date                                                             \n",
            "2002-08-12               NaN        -0.004188         -0.005304  \n",
            "2002-08-13               NaN         0.032632          0.004111  \n",
            "2002-08-14               NaN         0.030519          0.021886  \n",
            "2002-08-15               NaN         0.019900          0.007848  \n",
            "2002-08-16               NaN         0.001951          0.010550  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchial Risk Parity\n",
        "\n",
        "Make sure you replace the path:\n",
        "\n",
        "\n",
        "```\n",
        "csv_path = '/content/sample_data/diverseReturns.csv'\n",
        "```\n",
        "\n",
        "To the correct relative path to your historical returns data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6YItVmQYyABM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as mpl\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def getIVP(cov, **kargs):\n",
        "    # Compute the inverse-variance portfolio\n",
        "    ivp = 1. / np.diag(cov)\n",
        "    ivp /= ivp.sum()\n",
        "    return ivp\n",
        "\n",
        "def getClusterVar(cov, cItems):\n",
        "    # Compute variance per cluster\n",
        "    cov_ = cov.loc[cItems, cItems] # matrix slice\n",
        "    w_ = getIVP(cov_).reshape(-1, 1)\n",
        "    cVar = np.dot(np.dot(w_.T, cov_), w_)[0, 0]\n",
        "    return cVar\n",
        "\n",
        "def getQuasiDiag(link):\n",
        "    # Sort clustered items by distance\n",
        "    link = link.astype(int)\n",
        "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
        "    numItems = link[-1, 3] # number of original items\n",
        "    while sortIx.max() >= numItems:\n",
        "        sortIx.index = range(0, sortIx.shape[0] * 2, 2) # make space\n",
        "        df0 = sortIx[sortIx >= numItems]\n",
        "        i = df0.index\n",
        "        j = df0.values - numItems\n",
        "        sortIx[i] = link[j, 0] # item 1\n",
        "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
        "        sortIx = pd.concat([sortIx, df0]) # item 2 (modified to use concat)\n",
        "        sortIx = sortIx.sort_index() # re-sort\n",
        "        sortIx.index = range(sortIx.shape[0]) # re-index\n",
        "    return sortIx.tolist()\n",
        "\n",
        "\n",
        "def getRecBipart(cov, sortIx):\n",
        "    # Compute HRP allocation\n",
        "    w = pd.Series(1, index=sortIx)\n",
        "    cItems = [sortIx] # initialize all items in one cluster\n",
        "    while len(cItems) > 0:\n",
        "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]\n",
        "        for i in range(0, len(cItems), 2): # parse in pairs\n",
        "            cItems0 = cItems[i] # cluster 1\n",
        "            cItems1 = cItems[i + 1] # cluster 2\n",
        "            cVar0 = getClusterVar(cov, cItems0)\n",
        "            cVar1 = getClusterVar(cov, cItems1)\n",
        "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
        "            w[cItems0] *= alpha # weight 1\n",
        "            w[cItems1] *= 1 - alpha # weight 2\n",
        "    return w\n",
        "\n",
        "def correlDist(corr):\n",
        "    # Compute the correlation distance\n",
        "    dist = ((1 - corr) / 2.)**.5 # distance matrix\n",
        "    return dist\n",
        "\n",
        "def plotCorrMatrix(path, corr, labels=None):\n",
        "    # Heatmap of the correlation matrix\n",
        "    if labels is None:\n",
        "        labels = []\n",
        "    mpl.pcolor(corr)\n",
        "    mpl.colorbar()\n",
        "    mpl.yticks(np.arange(.5, corr.shape[0] + .5), labels)\n",
        "    mpl.xticks(np.arange(.5, corr.shape[0] + .5), labels)\n",
        "    mpl.savefig(path)\n",
        "    mpl.clf()\n",
        "    mpl.close() # reset pylab\n",
        "    return\n",
        "\n",
        "def generateDataFromCSV(csv_path):\n",
        "    # Read the data from the CSV file\n",
        "    data = pd.read_csv(csv_path)\n",
        "\n",
        "    return data\n",
        "\n",
        "def main():\n",
        "    nObs, size0, size1, sigma1 = 10000, 5, 5, .25\n",
        "\n",
        "    # Provide the path to your CSV file\n",
        "    csv_path = '/content/seventyFivePercentMerged'\n",
        "\n",
        "    # Call the modified generateData function\n",
        "    data = generateDataFromCSV(csv_path)\n",
        "    cols = [random.randint(0, size0 - 1) for i in range(size1)]\n",
        "    print([(j + 1, size0 + i) for i, j in enumerate(cols, 1)])\n",
        "    # 2) Compute and plot correl matrix\n",
        "    cov, corr = data.cov(), data.corr()\n",
        "    plotCorrMatrix('HRP3_corr0.png', corr, labels=corr.columns)\n",
        "    # 3) Cluster\n",
        "    dist = correlDist(corr)\n",
        "    link = sch.linkage(dist, 'single')\n",
        "    sortIx = getQuasiDiag(link)\n",
        "    sortIx = corr.index[sortIx].tolist() # recover labels\n",
        "    df0 = corr.loc[sortIx, sortIx] # reorder\n",
        "    plotCorrMatrix('HRP3_corr1.png', df0, labels=df0.columns)\n",
        "    # 4) Capital allocation\n",
        "    hrp = getRecBipart(cov, sortIx)\n",
        "\n",
        "    return print(hrp)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_G3G5mykYCZ",
        "outputId": "71868478-3009-4190-edf4-aad44ad46978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2, 6), (4, 7), (5, 8), (1, 9), (2, 10)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-883ee41ce843>:89: FutureWarning: The default value of numeric_only in DataFrame.cov is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  cov, corr = data.cov(), data.corr()\n",
            "<ipython-input-10-883ee41ce843>:89: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
            "  cov, corr = data.cov(), data.corr()\n",
            "<ipython-input-10-883ee41ce843>:93: ClusterWarning: scipy.cluster: The symmetric non-negative hollow observation matrix looks suspiciously like an uncondensed distance matrix\n",
            "  link = sch.linkage(dist, 'single')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daily_Return_HCLTECH    0.050082\n",
            "Daily_Return_INFY       0.068557\n",
            "Daily_Return_RMV        0.073394\n",
            "Daily_Return_GD         0.087791\n",
            "Daily_Return_DAL        0.023585\n",
            "Daily_Return_UPS        0.110376\n",
            "Daily_Return_V (1)      0.067870\n",
            "Daily_Return_EWZ        0.031774\n",
            "Daily_Return_MSFT       0.029497\n",
            "Daily_Return_ESGV       0.075575\n",
            "Daily_Return_EWT        0.066235\n",
            "Daily_Return_IEMG       0.256164\n",
            "Daily_Return_FL         0.028675\n",
            "Daily_Return_LULU       0.015289\n",
            "Daily_Return_UAA        0.015136\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the [backtester](https://www.portfoliovisualizer.com/backtest-portfolio#analysisResults) to test performance."
      ],
      "metadata": {
        "id": "Hu69McM-JZsN"
      }
    }
  ]
}